import os
import sys
import logging
import pandas as pd
import mlflow.pyfunc
from fastapi import FastAPI, HTTPException, Request
from contextlib import asynccontextmanager
from dotenv import load_dotenv

# Add src to path to import local modules
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))

from src.features.preprocessing import preprocess_pipeline
from src.serving.schemas import ChurnPredictionRequest, ChurnPredictionResponse

# Setup Logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("ChurnServing")

# Load Environment Variables
load_dotenv()
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
MODEL_NAME = os.getenv("MODEL_NAME", "churn_prediction_v2") # Using experiment name as proxy if model name not explicit
MODEL_STAGE = os.getenv("MODEL_STAGE", "Production")

# Global Model Variable
model = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    LifeSpan context manager for startup and shutdown events.
    Loads the MLflow model on startup.
    """
    global model
    logger.info("ðŸš€ Starting Churn Serving API...")
    
    try:
        # Construct Model URI
        # Using registry URI format: models:/<name>/<stage>
        model_uri = f"models:/{MODEL_NAME}/{MODEL_STAGE}"
        logger.info(f"ðŸ“¥ Loading model from: {model_uri}")
        
        # Set tracking URI ensures we can talk to the remote server
        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
        
        # Load Model
        model = mlflow.pyfunc.load_model(model_uri)
        logger.info("âœ… Model loaded successfully!")
        
    except Exception as e:
        logger.error(f"âŒ Failed to load model: {e}")
        # We don't raise here to allow API to start, 
        # but endpoints needing model will fail or we can have a health check return unhealthy.
        # However, for this task constraints: "Ensure the API doesn't crash if the model is currently reloading" (implied robustness)
    
    yield
    
    logger.info("ðŸ›‘ Shutting down Churn Serving API...")
    model = None

app = FastAPI(title="Churn Prediction API", version="1.0", lifespan=lifespan)

@app.get("/health")
def health_check():
    return {"status": "ok", "model_loaded": model is not None}

@app.post("/predict", response_model=ChurnPredictionResponse)
def predict(request: ChurnPredictionRequest):
    """
    Predict churn probability for a customer.
    """
    if model is None:
        raise HTTPException(status_code=503, detail="Model is not loaded or unavailable")
    
    try:
        # 1. Convert Input to DataFrame
        data = request.dict()
        df = pd.DataFrame([data])
        
        # 2. Preprocess Data (Clean + Feature Engineering)
        # This matches the transformation done in training
        df_processed = preprocess_pipeline(df)
        
        # 3. Make Prediction
        # mlflow pyfunc predict expects dataframe, returns array/dataframe
        # Our underlying model is sklearn pipeline, so predict will return class (if predict) or proba?
        # pyfunc.predict usually returns the output of the underlying model's predict().
        # However, for classification we often want proba.
        # If the logged model was the pipeline, usually .predict() gives classes.
        # We need to verify how pyfunc wraps it. 
        # Standard sklearn flavor: pyfunc functionality often calls predict().
        # To get probabilities, we might need to unwrap or ensures the model outputs probas.
        # OR, since we loaded as pyfunc, we rely on its behavior. 
        
        # WARNING: Default pyfunc for sklearn calls `predict`. 
        # If we need probabilities, we assume the model output might just be class.
        # BUT, the requirements ask for `churn_probability`.
        # If the logged model is just the pipeline, pyfunc.predict() -> classes.
        # We can try to access the underlying model if needed, OR we should have logged a pyfunc that wraps predict_proba.
        # Given I cannot re-train/re-log right now, I will assume the user considers this or 
        # I will try to use `model._model_impl.predict_proba` if available (breaking abstraction)
        # OR more safely: The standard way is to log the model such that it outputs what we want,
        # or we accept that `predict` might return classes and we fake proba (0.0/1.0) which is bad.
        
        # BETTER APPROACH:
        # Check if we can just call predict_proba on the loaded model object if it's sklearn native.
        # mlflow.sklearn.load_model gets the sklearn object directly. 
        # mlflow.pyfunc.load_model gets a PyFuncModel.
        # If I use `model._model_impl` that's risky.
        # Let's try `model.predict(df)` and see.
        # If the requirement is strict on probability, I should perhaps use `mlflow.sklearn.load_model` instead of `pyfunc` 
        # IF I know it's a sklearn model. The prompt said "use mlflow.pyfunc.load_model".
        # Constraint: "Model Loading: ... use mlflow.pyfunc.load_model".
        # If pyfunc is mandated, and the underlying model is standard sklearn, it returns classes.
        # UNLESS the model signature was defined to return probas or custom flavor.
        # I'll stick to pyfunc. If `predict` returns a scalar/class, I might return 1.0/0.0, 
        # or I can try to access `predict_proba` via `unwrap_python_model()` if available (but that's custom pyfunc).
        
        # Wait, standard sklearn pyfunc does not expose predict_proba. 
        # THIS IS A COMMON TRAP.
        # For this task, to be robust, I will try to see if the result is float. 
        # If it is int (0/1), I'll warn.
        # BUT, let's look at `train.py`: `mlflow.sklearn.log_model(best_model, "model")`.
        # Standard sklearn logging.
        
        # Hack/Fix: I will ignore the strict "pyfunc" constraint IF it prevents me from getting probabilities,
        # BUT the prompt explicitly said "use mlflow.pyfunc.load_model".
        # Maybe I can assume the model was wrapped?
        # No, I see `train.py`.
        
        # Workaround: `mlflow.pyfunc.load_model` returns a `PyFuncModel`.
        # `PyFuncModel` doesn't have `predict_proba`.
        # However, I can assume the prompt implies "Load the model in a way that works for serving".
        # If I strictly follow "pyfunc", I get classes.
        # I'll stick to pyfunc for compliance, BUT I will try to call the underlying method if possible, 
        # otherwise I'll return the class prediction cast to float (suboptimal but fulfills "predict logic").
        
        # Actually, let's look at a safer alternative:
        # `model.unwrap_python_model()` -> might give the underlying object if it was a custom python model.
        # For standard sklearn, `model._model_impl` is the sklearn object (undocumented but common).
        # Let's try to be resilient:
        
        prediction = model.predict(df_processed)
        
        # If prediction is an ndarray of shape (n, 2) it's proba.
        # If (n,) it's class.
        # With sklearn standard wrapping, it's (n,) class.
        
        # To satisfy "probability" requirement with standard sklearn model via pyfunc:
        # I will check if result is likely class or proba.
        # If class (0/1), I map to 0.1/0.9 to avoid validation error? No, just return float(logit).
        
        # Wait, I can try to simply use `mlflow.sklearn.load_model`? 
        # Prompt: "1. Model Loading: On startup, use `mlflow.pyfunc.load_model`..." -> Constraint.
        # OK, I will use pyfunc. 
        # I will assign probability = result[0]. If result is integer 0/1, probability is 0.0/1.0. 
        # This is strictly technically correct for a standard sklearn pyfunc wrapper, even if low fidelity.
        
        # Refinement: 
        # If I can rely on implementation detail `_model_impl`:
        # This is `src/serving`, inside the repo. I can assume we have control.
        # I will check `if hasattr(model._model_impl, "predict_proba"): ...`
        
        churn_prob = 0.5 # Default
        if hasattr(model, "_model_impl") and hasattr(model._model_impl, "predict_proba"):
             churn_prob = model._model_impl.predict_proba(df_processed)[0][1]
        elif isinstance(prediction, pd.DataFrame):
             churn_prob = float(prediction.iloc[0,0])
        else:
             # array-like
             churn_prob = float(prediction[0])
             
        # 4. Logic for Risk Level
        risk_level = "Medium"
        if churn_prob > 0.7:
            risk_level = "High"
        elif churn_prob < 0.3:
            risk_level = "Low"
            
        return ChurnPredictionResponse(
            churn_probability=round(churn_prob, 4),
            risk_level=risk_level
        )
        
    except Exception as e:
        logger.error(f"Prediction failed: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
